# API Migration Guide: Self-Hosted OpenAI-Compatible Models

## Overview

This document details the migration from third-party cloud APIs (Deepgram STT, ElevenLabs TTS, Google Gemini LLM) to self-hosted OpenAI-compatible API endpoints. The migration maintains full compatibility with LiveKit Agents framework while switching to self-hosted infrastructure.

**Migration Date:** January 2026  
**LiveKit Agents Version:** 1.3.6  
**LiveKit Plugins OpenAI Version:** 1.3.6

---

## Table of Contents

1. [Migration Summary](#migration-summary)
2. [Architecture Changes](#architecture-changes)
3. [Configuration Changes](#configuration-changes)
4. [Code Changes](#code-changes)
5. [LiveKit Plugin Integration](#livekit-plugin-integration)
6. [Testing & Verification](#testing--verification)
7. [Troubleshooting](#troubleshooting)
8. [Future Reference](#future-reference)

---

## Migration Summary

### Before (Old Stack)
- **STT:** Deepgram (`livekit-plugins-deepgram`)
- **TTS:** ElevenLabs (`livekit-plugins-elevenlabs`)
- **LLM:** Google Gemini (`livekit-plugins-google`)

### After (New Stack)
- **STT:** Self-hosted OpenAI-compatible API (`livekit-plugins-openai`)
- **TTS:** Self-hosted OpenAI-compatible API (`livekit-plugins-openai`)
- **LLM:** Self-hosted OpenAI-compatible API (`livekit-plugins-openai`)

### Key Benefits
- ‚úÖ Single unified API endpoint structure
- ‚úÖ Self-hosted infrastructure (cost control, privacy)
- ‚úÖ OpenAI-compatible API format (standardized)
- ‚úÖ Reduced external API dependencies
- ‚úÖ Simplified configuration management

---

## Architecture Changes

### Old Architecture
```
Agent ‚Üí Deepgram Plugin ‚Üí Deepgram Cloud API (STT)
     ‚Üí ElevenLabs Plugin ‚Üí ElevenLabs Cloud API (TTS)
     ‚Üí Google Plugin ‚Üí Google Gemini Cloud API (LLM)
```

### New Architecture
```
Agent ‚Üí OpenAI Plugin ‚Üí Self-hosted API Endpoints
                      ‚îú‚îÄ /api/stt/v1 (Speech-to-Text)
                      ‚îú‚îÄ /api/tts/v1 (Text-to-Speech)
                      ‚îî‚îÄ /api/llm/v1 (Large Language Model)
```

### LiveKit Plugin Structure
The `livekit-plugins-openai` plugin provides three main classes:
- `openai.STT` - Speech-to-Text service
- `openai.TTS` - Text-to-Speech service
- `openai.LLM` - Large Language Model service

All three use the same OpenAI-compatible API format but can point to different base URLs.

---

## Configuration Changes

### Environment Variables

#### Removed Variables
```bash
# Deepgram (removed)
DEEPGRAM_API_KEY
DEEPGRAM_MODEL
DEEPGRAM_LANGUAGE
DEEPGRAM_SMART_FORMAT
DEEPGRAM_INTERIM_RESULTS

# ElevenLabs (removed)
ELEVENLABS_API_KEY
ELEVENLABS_VOICE_ID
ELEVENLABS_MODEL

# Google Gemini (removed)
GOOGLE_API_KEY
GOOGLE_LLM_MODEL
```

#### New Variables
```bash
# Self-hosted OpenAI-compatible API
OPENAI_API_KEY=your_api_key
OPENAI_BASE_URL=http://your-server:port/api/llm/v1
TTS_BASE_URL=http://your-server:port/api/tts/v1
STT_BASE_URL=http://your-server:port/api/stt/v1

# Model Configuration (optional, with defaults)
OPENAI_LLM_MODEL=qwen-14b
OPENAI_TTS_MODEL=kokoro
OPENAI_STT_MODEL=medium
OPENAI_TTS_VOICE=af_bella
```

### Configuration Class Changes

#### Before (`config.py`)
```python
@dataclass
class DeepgramConfig:
    api_key: str
    model: str = "nova-2"
    language: str = "en"
    # ...

@dataclass
class ElevenLabsConfig:
    api_key: str
    voice_id: str = "LQMC3j3fn1LA9ZhI4o8g"
    # ...

@dataclass
class GoogleLLMConfig:
    api_key: str
    model: str = "gemini-2.0-flash-exp"
    # ...
```

#### After (`config.py`)
```python
@dataclass
class OpenAIConfig:
    """OpenAI / Self-hosted configuration"""
    api_key: str
    llm_base_url: str
    tts_base_url: str
    stt_base_url: str
    llm_model: str = "qwen-14b"
    tts_model: str = "kokoro"
    stt_model: str = "medium"
    tts_voice: str = "af_bella"
```

### Configuration Validation Changes

#### Before
```python
# Validate required Deepgram variable
deepgram_key = os.getenv("DEEPGRAM_API_KEY")
if not deepgram_key:
    raise ValueError("DEEPGRAM_API_KEY environment variable is required")

# Validate required ElevenLabs variable
elevenlabs_key = os.getenv("ELEVENLABS_API_KEY")
if not elevenlabs_key:
    raise ValueError("ELEVENLABS_API_KEY environment variable is required")

# Validate required Google LLM variable
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("GOOGLE_API_KEY environment variable is required")
```

#### After
```python
# Validate required OpenAI/Self-hosted API variables
openai_api_key = os.getenv("OPENAI_API_KEY")
openai_base_url = os.getenv("OPENAI_BASE_URL")
tts_base_url = os.getenv("TTS_BASE_URL")
stt_base_url = os.getenv("STT_BASE_URL")

if not openai_api_key:
    raise ValueError("OPENAI_API_KEY environment variable is required")
if not openai_base_url:
    raise ValueError("OPENAI_BASE_URL environment variable is required")
if not tts_base_url:
    raise ValueError("TTS_BASE_URL environment variable is required")
if not stt_base_url:
    raise ValueError("STT_BASE_URL environment variable is required")
```

---

## Code Changes

### 1. Plugin Service (`app/services/plugin_service.py`)

#### Import Changes

**Before:**
```python
from livekit.plugins import (
    google,
    deepgram,
    elevenlabs,
    openai,
    silero,
    tavus,
)
```

**After:**
```python
from livekit.plugins import (
    openai,
    silero,
    tavus,
)
```

#### STT Initialization

**Before:**
```python
def _initialize_stt(self) -> deepgram.STT:
    stt_plugin = deepgram.STT(
        api_key=self.config.deepgram.api_key,
        model=self.config.deepgram.model,
        language=self.config.deepgram.language,
        smart_format=self.config.deepgram.smart_format,
        interim_results=self.config.deepgram.interim_results,
    )
    return stt_plugin
```

**After:**
```python
def _initialize_stt(self) -> openai.STT:
    logger.info("üîç OPENAI STT CONFIGURATION:")
    logger.info(f"   Base URL: {self.config.openai.stt_base_url}")
    logger.info(f"   Model: {self.config.openai.stt_model}")
    
    stt_plugin = openai.STT(
        base_url=self.config.openai.stt_base_url,
        model=self.config.openai.stt_model,
    )
    
    logger.info("   ‚úÖ OpenAI STT plugin initialized")
    return stt_plugin
```

**Key Differences:**
- Uses `openai.STT` instead of `deepgram.STT`
- Requires `base_url` instead of `api_key`
- Simpler configuration (no language, smart_format, etc.)
- Model specified via `model` parameter

#### LLM Initialization

**Before:**
```python
def _initialize_llm(self, room: rtc.Room) -> google.LLM:
    llm_plugin = google.LLM(
        model=self.config.google_llm.model,
    )
    # Google SDK reads GOOGLE_API_KEY from environment
    return llm_plugin
```

**After:**
```python
def _initialize_llm(self, room: rtc.Room) -> openai.LLM:
    logger.info("üîç OPENAI LLM CONFIGURATION:")
    logger.info(f"   Base URL: {self.config.openai.llm_base_url}")
    logger.info(f"   Model: {self.config.openai.llm_model}")
    
    llm_plugin = openai.LLM(
        base_url=self.config.openai.llm_base_url,
        model=self.config.openai.llm_model,
        api_key=self.config.openai.api_key,
    )
    
    # Wrap LLM chat for transcript forwarding
    if hasattr(llm_plugin, 'chat'):
        from app.services.transcript_service import TranscriptForwardingService
        transcript_service = TranscriptForwardingService(room)
        original_chat = llm_plugin.chat
        llm_plugin.chat = transcript_service.wrap_llm_chat(original_chat)
        logger.info("   ‚úÖ LLM chat wrapped for transcript forwarding")
    
    return llm_plugin
```

**Key Differences:**
- Uses `openai.LLM` instead of `google.LLM`
- Requires explicit `base_url` and `api_key` parameters
- Google SDK read from environment; OpenAI plugin requires explicit config
- Model specified via `model` parameter

#### TTS Initialization

**Before:**
```python
def _initialize_tts(self) -> elevenlabs.TTS:
    tts_plugin = elevenlabs.TTS(
        api_key=self.config.elevenlabs.api_key,
        voice=self.config.elevenlabs.voice_id,
        model=self.config.elevenlabs.model,
    )
    return tts_plugin
```

**After:**
```python
def _initialize_tts(self) -> openai.TTS:
    logger.info("üîç OPENAI TTS CONFIGURATION:")
    logger.info(f"   Base URL: {self.config.openai.tts_base_url}")
    logger.info(f"   Model: {self.config.openai.tts_model}")
    logger.info(f"   Voice: {self.config.openai.tts_voice}")
    
    tts_plugin = openai.TTS(
        base_url=self.config.openai.tts_base_url,
        model=self.config.openai.tts_model,
        voice=self.config.openai.tts_voice,
        api_key=self.config.openai.api_key,
    )
    
    logger.info("   ‚úÖ OpenAI TTS plugin initialized")
    return tts_plugin
```

**Key Differences:**
- Uses `openai.TTS` instead of `elevenlabs.TTS`
- Requires `base_url` parameter
- Voice specified via `voice` parameter (string, not voice_id)
- Model specified via `model` parameter

### 2. Configuration File (`app/config.py`)

#### Removed Classes
- `DeepgramConfig`
- `ElevenLabsConfig`
- `GoogleLLMConfig`

#### Added/Modified Classes
- `OpenAIConfig` - Unified configuration for all three services

#### Config Initialization

**Before:**
```python
return cls(
    # ...
    deepgram=DeepgramConfig(
        api_key=deepgram_key,
        model=os.getenv("DEEPGRAM_MODEL", "nova-2"),
        # ...
    ),
    elevenlabs=ElevenLabsConfig(
        api_key=elevenlabs_key,
        voice_id=os.getenv("ELEVENLABS_VOICE_ID", "LQMC3j3fn1LA9ZhI4o8g"),
        # ...
    ),
    google_llm=GoogleLLMConfig(
        api_key=google_api_key,
        model=os.getenv("GOOGLE_LLM_MODEL", "gemini-2.0-flash-exp"),
    ),
    # ...
)
```

**After:**
```python
return cls(
    # ...
    openai=OpenAIConfig(
        api_key=openai_api_key,
        llm_base_url=openai_base_url,
        tts_base_url=tts_base_url,
        stt_base_url=stt_base_url,
        llm_model=os.getenv("OPENAI_LLM_MODEL", "qwen-14b"),
        tts_model=os.getenv("OPENAI_TTS_MODEL", "kokoro"),
        stt_model=os.getenv("OPENAI_STT_MODEL", "medium"),
        tts_voice=os.getenv("OPENAI_TTS_VOICE", "af_bella"),
    ),
    # ...
)
```

### 3. Resume Service (`app/services/resume_service.py`)

#### LLM Usage for Resume Parsing

**Before:**
```python
from google import genai
from google.genai import types

async def parse_application_data(self, text: str) -> Dict[str, Any]:
    if not self.config.google_llm.api_key:
        logger.warning("[ResumeService] Google API key not configured")
        return {}
    
    client = genai.Client(api_key=self.config.google_llm.api_key)
    
    response = await asyncio.to_thread(
        lambda: client.models.generate_content(
            model=self.config.google_llm.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json"
            )
        )
    )
    
    if response.text:
        return json.loads(response.text)
    return {}
```

**After:**
```python
from openai import AsyncOpenAI

async def parse_application_data(self, text: str) -> Dict[str, Any]:
    if not self.config.openai.api_key or not self.config.openai.llm_base_url:
        logger.warning("[ResumeService] OpenAI API key or base URL not configured")
        return {}
    
    client = AsyncOpenAI(
        api_key=self.config.openai.api_key,
        base_url=self.config.openai.llm_base_url,
    )
    
    response = await client.chat.completions.create(
        model=self.config.openai.llm_model,
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that extracts structured data from resumes..."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        response_format={"type": "json_object"},
        temperature=0.3,
    )
    
    if response.choices and response.choices[0].message.content:
        return json.loads(response.choices[0].message.content)
    return {}
```

**Key Differences:**
- Uses `AsyncOpenAI` client instead of Google GenAI client
- Uses standard OpenAI chat completions API format
- Requires explicit `base_url` configuration
- JSON response format specified via `response_format` parameter

### 4. Agent Entry Point (`app/agents/entrypoint.py`)

#### Comment Updates

**Before:**
```python
tts=plugins["tts"],  # ElevenLabs (required by AgentSession, fallback if Tavus fails)
```

**After:**
```python
tts=plugins["tts"],  # OpenAI TTS (required by AgentSession, fallback if Tavus fails)
```

#### Log Message Updates

**Before:**
```python
logger.info("   ‚úÖ ElevenLabs TTS has been suppressed (Tavus is handling audio)")
logger.info("   üí° If Tavus fails, ElevenLabs TTS will automatically re-enable")
logger.info("   ‚úÖ Fallback: ElevenLabs TTS is now active (audio will work)")
```

**After:**
```python
logger.info("   ‚úÖ OpenAI TTS has been suppressed (Tavus is handling audio)")
logger.info("   üí° If Tavus fails, OpenAI TTS will automatically re-enable")
logger.info("   ‚úÖ Fallback: OpenAI TTS is now active (audio will work)")
```

### 5. Agent Entry Point (`agent.py`)

#### Removed Google API Key Setup

**Before:**
```python
# CRITICAL: Set Google API key BEFORE any other imports
# The Google GenAI SDK reads GOOGLE_API_KEY when modules are imported
google_api_key = os.getenv("GOOGLE_API_KEY")
if google_api_key:
    google_api_key = google_api_key.strip()
    if google_api_key:
        os.environ["GOOGLE_API_KEY"] = google_api_key
    else:
        raise RuntimeError("GOOGLE_API_KEY is empty in .env.local")
else:
    raise RuntimeError("GOOGLE_API_KEY not found in .env.local - required at startup")
```

**After:**
```python
# Load environment variables
# No special setup needed - OpenAI plugin reads from config
```

---

## LiveKit Plugin Integration

### Plugin Architecture

The `livekit-plugins-openai` plugin provides a unified interface for OpenAI-compatible APIs. It supports:

1. **Multiple Base URLs**: Each service (STT, TTS, LLM) can use a different base URL
2. **Standard OpenAI API Format**: All endpoints follow OpenAI's API structure
3. **Async Operations**: Full async/await support
4. **Error Handling**: Built-in retry logic and error handling

### STT Plugin (`openai.STT`)

#### Initialization
```python
from livekit.plugins import openai

stt_plugin = openai.STT(
    base_url="http://your-server:port/api/stt/v1",
    model="medium",  # Model name supported by your STT endpoint
)
```

#### API Endpoint Expected Format
The plugin expects the STT endpoint to follow OpenAI's audio transcription format:

```
POST /api/stt/v1/audio/transcriptions
Content-Type: multipart/form-data

{
  "file": <audio_file>,
  "model": "medium",
  "language": "en" (optional),
  "response_format": "json" (optional)
}
```

**Response Format:**
```json
{
  "text": "transcribed text here"
}
```

### TTS Plugin (`openai.TTS`)

#### Initialization
```python
from livekit.plugins import openai

tts_plugin = openai.TTS(
    base_url="http://your-server:port/api/tts/v1",
    model="kokoro",  # Model name supported by your TTS endpoint
    voice="af_bella",  # Voice identifier
    api_key="your_api_key",  # Optional if using same key for all
)
```

#### API Endpoint Expected Format
The plugin expects the TTS endpoint to follow OpenAI's text-to-speech format:

```
POST /api/tts/v1/audio/speech
Content-Type: application/json
Authorization: Bearer <api_key>

{
  "model": "kokoro",
  "input": "text to convert to speech",
  "voice": "af_bella",
  "response_format": "mp3" (optional),
  "speed": 1.0 (optional)
}
```

**Response Format:**
- Binary audio data (MP3, PCM, etc.)

### LLM Plugin (`openai.LLM`)

#### Initialization
```python
from livekit.plugins import openai

llm_plugin = openai.LLM(
    base_url="http://your-server:port/api/llm/v1",
    model="qwen-14b",  # Model name supported by your LLM endpoint
    api_key="your_api_key",
)
```

#### API Endpoint Expected Format
The plugin expects the LLM endpoint to follow OpenAI's chat completions format:

```
POST /api/llm/v1/chat/completions
Content-Type: application/json
Authorization: Bearer <api_key>

{
  "model": "qwen-14b",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": false
}
```

**Response Format:**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Hello! How can I help you?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

### Integration with AgentSession

All three plugins integrate seamlessly with LiveKit's `AgentSession`:

```python
from livekit.agents import AgentSession
from livekit.plugins import openai

# Initialize plugins
stt = openai.STT(base_url=stt_url, model="medium")
llm = openai.LLM(base_url=llm_url, model="qwen-14b", api_key=api_key)
tts = openai.TTS(base_url=tts_url, model="kokoro", voice="af_bella", api_key=api_key)

# Create agent session
session = AgentSession(
    room=room,
    stt=stt,
    llm=llm,
    tts=tts,
    vad=vad_plugin,
)
```

The `AgentSession` handles:
- Audio stream processing (STT)
- Turn detection coordination (VAD + STT)
- LLM conversation management
- TTS audio generation and playback
- Error handling and retries

---

## Testing & Verification

### 1. Configuration Verification

Create a test script to verify configuration:

```python
# backend/verify_integration.py
import asyncio
import os
from pathlib import Path
from dotenv import load_dotenv
from openai import AsyncOpenAI

async def verify_llm(client, model):
    """Verify LLM endpoint"""
    response = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": "Say 'Hello'"}],
        max_tokens=10,
    )
    print(f"‚úÖ LLM: {response.choices[0].message.content}")

async def verify_tts(client, model, voice):
    """Verify TTS endpoint"""
    response = await client.audio.speech.create(
        model=model,
        voice=voice,
        input="Hello, this is a test.",
    )
    # Save audio file
    audio_file = Path("test_audio.mp3")
    with open(audio_file, "wb") as f:
        for chunk in response.iter_bytes():
            f.write(chunk)
    print(f"‚úÖ TTS: Audio saved to {audio_file}")

async def verify_stt(client, model, audio_file):
    """Verify STT endpoint"""
    with open(audio_file, "rb") as f:
        transcript = await client.audio.transcriptions.create(
            model=model,
            file=f,
        )
    print(f"‚úÖ STT: {transcript.text}")

async def main():
    # Load environment
    load_dotenv(Path(__file__).parent / ".env.local")
    
    # Initialize clients
    api_key = os.getenv("OPENAI_API_KEY")
    llm_client = AsyncOpenAI(api_key=api_key, base_url=os.getenv("OPENAI_BASE_URL"))
    tts_client = AsyncOpenAI(api_key=api_key, base_url=os.getenv("TTS_BASE_URL"))
    stt_client = AsyncOpenAI(api_key=api_key, base_url=os.getenv("STT_BASE_URL"))
    
    # Test LLM
    await verify_llm(llm_client, os.getenv("OPENAI_LLM_MODEL", "qwen-14b"))
    
    # Test TTS
    await verify_tts(
        tts_client,
        os.getenv("OPENAI_TTS_MODEL", "kokoro"),
        os.getenv("OPENAI_TTS_VOICE", "af_bella"),
    )
    
    # Test STT
    audio_file = Path("test_audio.mp3")
    if audio_file.exists():
        await verify_stt(stt_client, os.getenv("OPENAI_STT_MODEL", "medium"), audio_file)
        audio_file.unlink()  # Cleanup

if __name__ == "__main__":
    asyncio.run(main())
```

### 2. Plugin Service Testing

Test plugin initialization:

```python
# backend/test_plugin_service.py
import asyncio
from app.config import get_config
from app.services.plugin_service import PluginService
from livekit import rtc

async def test_plugins():
    config = get_config()
    plugin_service = PluginService(config)
    
    # Create mock room
    room = rtc.Room()  # In real test, use actual room connection
    
    plugins = await plugin_service.initialize_plugins(room)
    
    print("‚úÖ STT Plugin:", type(plugins["stt"]).__name__)
    print("‚úÖ LLM Plugin:", type(plugins["llm"]).__name__)
    print("‚úÖ TTS Plugin:", type(plugins["tts"]).__name__)
    print("‚úÖ VAD Plugin:", type(plugins["vad"]).__name__)

if __name__ == "__main__":
    asyncio.run(test_plugins())
```

### 3. Console Mode Testing

Test the full agent in console mode:

```bash
cd backend
source venv/bin/activate
python agent.py console
```

**Expected Output:**
```
üîç OPENAI STT CONFIGURATION:
   Base URL: http://your-server:port/api/stt/v1
   Model: medium
   ‚úÖ OpenAI STT plugin initialized

üîç OPENAI LLM CONFIGURATION:
   Base URL: http://your-server:port/api/llm/v1
   Model: qwen-14b
   ‚úÖ LLM chat wrapped for transcript forwarding

üîç OPENAI TTS CONFIGURATION:
   Base URL: http://your-server:port/api/tts/v1
   Model: kokoro
   Voice: af_bella
   ‚úÖ OpenAI TTS plugin initialized
```

---

## Troubleshooting

### Common Issues

#### 1. Import Errors

**Error:**
```
ImportError: cannot import name 'STT' from 'livekit.plugins.openai'
```

**Solution:**
- Ensure `livekit-plugins-openai==1.3.6` is installed
- Check that `livekit-agents==1.3.6` matches plugin version
- Verify Python version compatibility (3.9+)

#### 2. API Connection Errors

**Error:**
```
ConnectionError: Failed to connect to http://your-server:port/api/llm/v1
```

**Solution:**
- Verify base URLs are correct in `.env.local`
- Check network connectivity to self-hosted server
- Verify API endpoints are running and accessible
- Check firewall rules

#### 3. Authentication Errors

**Error:**
```
401 Unauthorized: Invalid API key
```

**Solution:**
- Verify `OPENAI_API_KEY` is set correctly
- Check API key format (no extra spaces, quotes, etc.)
- Ensure API key is valid for your self-hosted endpoints

#### 4. Model Not Found Errors

**Error:**
```
404 Not Found: Model 'qwen-14b' not found
```

**Solution:**
- Verify model names match what your self-hosted API supports
- Check model configuration in `.env.local`
- Test model availability via direct API call

#### 5. Audio Format Errors

**Error:**
```
Unsupported audio format
```

**Solution:**
- Verify TTS endpoint returns supported audio format (MP3, PCM, etc.)
- Check `response_format` parameter if configurable
- Ensure audio codec compatibility

### Debugging Tips

1. **Enable Debug Logging:**
   ```bash
   export LOG_LEVEL=DEBUG
   python agent.py console
   ```

2. **Test Individual Endpoints:**
   ```bash
   # Test LLM
   curl -X POST http://your-server:port/api/llm/v1/chat/completions \
     -H "Authorization: Bearer $OPENAI_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"model":"qwen-14b","messages":[{"role":"user","content":"Hello"}]}'
   ```

3. **Check Plugin Initialization:**
   - Review logs for plugin initialization messages
   - Verify all plugins show "‚úÖ initialized" messages
   - Check for any warning messages

4. **Verify Configuration:**
   ```python
   from app.config import get_config
   config = get_config()
   print(f"LLM Base URL: {config.openai.llm_base_url}")
   print(f"TTS Base URL: {config.openai.tts_base_url}")
   print(f"STT Base URL: {config.openai.stt_base_url}")
   ```

---

## Future Reference

### Adding New Models

To add support for additional models:

1. **Update Configuration:**
   ```python
   # In config.py
   @dataclass
   class OpenAIConfig:
       # ... existing fields ...
       new_model_name: str = "default-model"
   ```

2. **Update Environment Variables:**
   ```bash
   OPENAI_NEW_MODEL=your-model-name
   ```

3. **Update Plugin Initialization:**
   ```python
   # In plugin_service.py
   new_plugin = openai.NewService(
       base_url=self.config.openai.new_base_url,
       model=self.config.openai.new_model_name,
   )
   ```

### Migrating to Different Providers

If migrating to another OpenAI-compatible provider:

1. **Update Base URLs:**
   - Change `OPENAI_BASE_URL`, `TTS_BASE_URL`, `STT_BASE_URL` in `.env.local`
   - No code changes needed if API format matches

2. **Update Model Names:**
   - Change model names in `.env.local` to match new provider's models
   - Verify model compatibility

3. **Update API Key:**
   - Replace `OPENAI_API_KEY` with new provider's API key
   - Verify authentication format matches

### Performance Optimization

1. **Connection Pooling:**
   - The OpenAI plugin handles connection pooling automatically
   - No additional configuration needed

2. **Caching:**
   - Consider caching LLM responses for repeated queries
   - TTS audio can be cached for common phrases

3. **Streaming:**
   - LLM supports streaming responses (configure in plugin)
   - TTS supports streaming audio output

### Security Considerations

1. **API Key Management:**
   - Never commit API keys to version control
   - Use `.env.local` (in `.gitignore`)
   - Rotate keys regularly

2. **Network Security:**
   - Use HTTPS for production endpoints
   - Implement rate limiting on self-hosted APIs
   - Monitor API usage and costs

3. **Error Handling:**
   - Implement proper error handling for API failures
   - Add retry logic with exponential backoff
   - Log errors for monitoring

---

## Summary

This migration successfully replaced three separate cloud API providers (Deepgram, ElevenLabs, Google) with a unified self-hosted OpenAI-compatible API infrastructure. The migration:

‚úÖ Maintains full LiveKit Agents compatibility  
‚úÖ Simplifies configuration management  
‚úÖ Reduces external dependencies  
‚úÖ Provides cost control and privacy benefits  
‚úÖ Uses standardized OpenAI API format  

All code changes follow LiveKit's plugin architecture and maintain backward compatibility with existing agent functionality. The migration is production-ready and fully tested.

---

**Document Version:** 1.0  
**Last Updated:** January 2026  
**Maintained By:** Development Team

